{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experimentation (Original).ipynb","provenance":[{"file_id":"1shq6DTHqJsUvI_LqZEE0YW-vfdUM1tg5","timestamp":1569003868101}],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PE3VOu7H3WmE","colab_type":"code","outputId":"0478e976-af4f-402d-c17b-033babb352ee","executionInfo":{"status":"ok","timestamp":1574815616677,"user_tz":480,"elapsed":24293,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sGqiRSCczNzj","colab_type":"text"},"source":["# Additional Experiments"]},{"cell_type":"code","metadata":{"id":"ZM8P_kqKzNeT","colab_type":"code","outputId":"1a61e4d0-4f8b-47c4-f8ac-955ccf25f131","executionInfo":{"status":"ok","timestamp":1574816061414,"user_tz":480,"elapsed":7213,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":108}},"source":["# average word syllable count\n","\n","!pip install nltk\n","import nltk\n","nltk.download('punkt')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Metrics/readability')\n","import readability, io\n","\n","system = 'Reference'\n","\n","os.chdir('/content/drive/My Drive/Metrics/dress/all-system-output/Newsela/test/')\n","f = io.open(system, mode=\"r\", encoding=\"utf-8\")\n","contents = f.read()\n","rd = readability.Readability(contents)\n","sentences, words, syllables = rd.RawCounts()\n","score = rd.FleschKincaidGradeLevel()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python2.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk) (1.12.0)\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EmK7FW9aYY27","colab_type":"code","outputId":"1f68b521-039c-4b91-ea18-343df2bb57a4","executionInfo":{"status":"ok","timestamp":1574816071976,"user_tz":480,"elapsed":1454,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["score, words/sentences, syllables/words"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3.9926, 12.740947075208913, 1.2629354321527475)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"p39ZX0xR6q6t","colab_type":"code","outputId":"5dff2ab4-e729-40f9-c92a-172acbf8dc63","executionInfo":{"status":"ok","timestamp":1574816089475,"user_tz":480,"elapsed":1620,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import os\n","os.chdir('/content/drive/My Drive/Metrics/readability')\n","import readability, io\n","\n","system = 'Reference'\n","os.chdir('/content/drive/My Drive/Metrics/dress/all-system-output/Newsela/test/')\n","orig = io.open('Complex', mode=\"r\", encoding=\"utf-8\")\n","out = io.open(system, mode=\"r\", encoding=\"utf-8\")\n","orig_content = orig.read().splitlines()\n","out_content = out.read().splitlines()\n","numsentences = len(orig_content)\n","counter = 0\n","for i in range(numsentences):\n","  if out_content[i].count('.') > orig_content[i].count('.'):\n","    counter+=1\n","float(counter)/numsentences*100"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.8570102135561743"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"xuwBvGO0C9wc","colab_type":"code","colab":{}},"source":["percents = [0.0, 0.5, 1]\n","counts = [[0 for i in range(3)] for j in range(3)]\n","\n","import os\n","os.chdir('/content/drive/My Drive/Metrics/')\n","f = io.open('./dress/all-system-output/Newsela/test/Dress-Ls', mode=\"r\", encoding=\"utf-8\")\n","contents = f.read().splitlines()\n","numsentences = len(contents)\n","\n","for k in range(3):\n","  percent = percents[k]\n","\n","  # 30 files\n","  from random import shuffle, randint\n","  \n","  for n in range(30): # repeat 100 times for each corpus/system\n","    markers = [1]*int(numsentences*percent) + [0]*(numsentences-int(numsentences*percent))\n","    shuffle(markers)\n","\n","    with open('./100files/' + str(n) + '.txt', 'w+') as file:\n","      for i in range(numsentences):\n","        sentence = contents[i]\n","        words = contents[i].split(' ')\n","        if markers[i] == 1 and len(words) > 1:\n","          sentence = \"\"\n","          where = randint(1,len(words)-1)\n","          for j in range(len(words)):\n","            if j == where:\n","              sentence += \"the \"\n","            sentence += words[j] + \" \"\n","        file.write(sentence + '\\n')\n","\n","  for n in range(30):\n","    path = './100files/' + str(n) + '.txt'\n","    f = io.open(path, mode=\"r\", encoding=\"utf-8\")\n","    rd = readability.Readability(f.read())\n","    sentences, words, syllables = rd.RawCounts()\n","    counts[k][0] += (sentences/30)\n","    counts[k][1] += (words/30)\n","    counts[k][2] += (syllables/30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5slIFLRSdX62","colab_type":"code","outputId":"7b393e88-24e9-4578-9270-6716fa7315f7","executionInfo":{"status":"ok","timestamp":1573426618701,"user_tz":480,"elapsed":15816,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["counts"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1079.0000000000007, 15528.999999999998, 18865.999999999996],\n"," [1079.0000000000007, 16069.766666666666, 18865.999999999996],\n"," [1079.0000000000007, 16610.5, 18865.999999999996]]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"hsAx7z_B5Otc","colab_type":"code","colab":{}},"source":["# average syllable count of longest word in a sentence\n","\n","import os, io\n","os.chdir('/content/drive/My Drive/Metrics/dress/all-system-output/Newsela/test/')\n","f = io.open('Dress-Ls', mode=\"r\", encoding=\"utf-8\")\n","contents = f.read().splitlines()\n","numsentences = len(contents)\n","\n","with open('/content/drive/My Drive/Metrics/test.txt', 'w+') as file:\n","  sum = 0\n","  for i in range(numsentences):\n","    words = contents[i].split(' ')\n","    file.write(words[find_longest(words)]+'\\n')\n","    \n","f = io.open('/content/drive/My Drive/Metrics/test.txt', mode=\"r\", encoding=\"utf-8\")\n","contents = f.read()\n","rd = readability.Readability(contents)\n","sentences, words, syllables = rd.RawCounts()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"btbokY_LGAdG","colab_type":"code","outputId":"6c409550-7331-4675-ee4b-cc9b78772109","executionInfo":{"status":"ok","timestamp":1571003495987,"user_tz":420,"elapsed":817,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["syllables"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3283.0"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"XLXGB6MOOnot","colab_type":"code","outputId":"06183191-6bd7-4d92-e7fd-b3d9a68f0f9a","executionInfo":{"status":"ok","timestamp":1571090775222,"user_tz":420,"elapsed":3673,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import os\n","os.chdir('/content/drive/My Drive/Metrics/')\n","\n","corpus = 'Newsela'\n","system = 'Dress-Ls'\n","path = \"./dress/all-system-output/\" + corpus + \"/test/\" + system\n","f = open(path, mode=\"r\")\n","contents = f.read().splitlines()\n","numsentences = len(contents)\n","#BLEU('Newsela', path)\n","longestthe(system, contents, numsentences)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([0.4468, 0.2339, 0.1581, 0.1074], [0.015, 0.0463, 0.8295])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"AFrgLBzM9XsO","colab_type":"text"},"source":["# Modify Output"]},{"cell_type":"code","metadata":{"id":"K3cEDUV_9W5j","colab_type":"code","outputId":"205cf4d7-45be-46bb-99b4-e30ef46cdcc9","executionInfo":{"status":"ok","timestamp":1573085999663,"user_tz":480,"elapsed":15312,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","os.chdir('/content/drive/My Drive/Metrics/')\n","\n","corpus = 'Newsela'\n","\n","systems = ['Dress-Ls'] #, 'Dress', 'PBMT-R', 'Hybrid']\n","for system in systems:\n","  path = \"./dress/all-system-output/\" + corpus + \"/test/\" + system\n","  f = open(path, mode=\"r\")\n","  contents = f.read().splitlines()\n","  numsentences = len(contents)\n","  print(numsentences)\n","  result = insert_period(system, contents, numsentences) #insert_period, insertthe, longestthe"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1077\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t_McJwvUGvWm","colab_type":"code","outputId":"64da1901-cbd3-4ce4-f729-25ecbc6304a9","executionInfo":{"status":"ok","timestamp":1573086054817,"user_tz":480,"elapsed":168,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["result"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, 0, 0, 0, 0, 0],\n"," [0.87668,\n","  0.24744999999999998,\n","  0.6464000000000001,\n","  7.735590000000002,\n","  9.8374,\n","  4.444],\n"," [0.43032000000000004,\n","  0.28580000000000005,\n","  0.39680000000000004,\n","  6.19264,\n","  9.895450000000002,\n","  5.6180900000000005],\n"," [0.25762, 0.20186999999999997, 0.30527, 5.3132, 9.94019, 6.4346],\n"," [0.16199,\n","  0.14300000000000002,\n","  0.23891000000000004,\n","  4.56282,\n","  9.975950000000001,\n","  7.11098]]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"7MyG5QTfqzup","colab_type":"code","colab":{}},"source":["def find_longest(words):\n","  \"\"\"Return index of the longest word in the given list\"\"\"\n","  maxLength = 0\n","  index = -1\n","  for i in range(len(words)):\n","    if len(words[i]) > maxLength:\n","      maxLength = len(words[i])\n","      index = i\n","  return index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqVPSFTznNhe","colab_type":"code","colab":{}},"source":["# Random splitting with period + replacing longest word with the\n","# DON'T RUN THIS CELL IF YOU DON'T WANT TO GENERATE NEW SET OF OUTPUT\n","\n","def period_longest(system, contents, numsentences):\n","  percents = [1] #[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n","  for percent in percents:\n","\n","    # 100 files\n","    from random import shuffle, randint\n","\n","    with open('./100files/current.txt', 'w+') as file:\n","      file.write('random full stop + longest word replaced\\n' + corpus + '\\n' + system + '\\n' + str(percent))\n","\n","    for n in range(1): # repeat 100 times for each corpus/system\n","      markers = [1]*int(numsentences*percent) + [0]*(numsentences-int(numsentences*percent))\n","      shuffle(markers)\n","\n","      with open('./100files/' + str(n) + '.txt', 'w+') as file:\n","        for i in range(numsentences):\n","          sentence = contents[i]\n","          words = contents[i].split(' ')\n","          if markers[i] == 1 and len(words) > 1:\n","            sentence = \"\"\n","            longest = find_longest(words)\n","            where = randint(1,len(words)-1)\n","            for j in range(len(words)):\n","              if j == where:\n","                  sentence += \". \"\n","              if j == longest:\n","                sentence += \"the \"\n","              else:\n","                sentence += words[j] + \" \"\n","          file.write(sentence + '\\n')\n","    \n","    path = './100files/0.txt'\n","    return BLEU(corpus, path)[1], SARI(corpus, path)[1] # detailed stats\n","  \n","    # run 100 rounds\n","    '''fkgl = []\n","    bleu = []\n","    sari = []\n","    import os\n","    os.chdir('/content/drive/My Drive/Metrics/')\n","\n","    for n in range(100):\n","      path = './100files/' + str(n) + '.txt'\n","      fkgl.append(FKGL(path))\n","      bleu.append(BLEU(corpus, path)[0])\n","      sari.append(SARI(corpus, path)[0])\n","\n","    # output to file\n","    import pandas as pd\n","    result = pd.DataFrame(list(zip(fkgl, bleu, sari)), columns = ['fkgl', 'bleu', 'sari'])\n","    result.to_csv('./period_longest_results/' + corpus + system + str(int(percent*100)) + '.csv')'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"14c7Hu4FU8Y3","colab_type":"code","colab":{}},"source":["# Random splitting with period\n","# DON'T RUN THIS CELL IF YOU DON'T WANT TO GENERATE NEW SET OF OUTPUT\n","\n","def insert_period(system, contents, numsentences):\n","  percents = [1] #[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n","  for percent in percents:\n","\n","    # 100 files\n","    from random import shuffle, randint\n","\n","    with open('./100files/current.txt', 'w+') as file:\n","      file.write('random full stop\\n' + corpus + '\\n' + system + '\\n' + str(percent))\n","\n","    for n in range(10): # repeat 100 times for each corpus/system\n","      markers = [1]*int(numsentences*percent) + [0]*(numsentences-int(numsentences*percent))\n","      shuffle(markers)\n","\n","      with open('./100files/' + str(n) + '.txt', 'w+') as file:\n","        for i in range(numsentences):\n","          sentence = contents[i]\n","          words = contents[i].split(' ')\n","          if markers[i] == 1 and len(words) > 1:\n","            sentence = \"\"\n","            where = randint(1,len(words)-2) # leave at least one word in between\n","            for j in range(len(words)):\n","              if j == where:\n","                sentence += \". \"\n","              sentence += words[j] + \" \"\n","          file.write(sentence + '\\n')\n","\n","    outputs = [[0 for x in range(6)] for y in range(5)] \n","    for n in range(10):\n","      path = './100files/' + str(n) + '.txt'\n","      out = SARI(corpus, path) # detailed stats\n","      for i in range(1,5):\n","        for j in range(6):\n","          outputs[i][j] += out[i][j]\n","    \n","    for i in range(1,5):\n","        for j in range(6):\n","          outputs[i][j] += out[i][j]/10\n","    return outputs\n","  \n","    '''\n","    # run 100 rounds\n","    fkgl = []\n","    bleu = []\n","    sari = []\n","    import os\n","    os.chdir('/content/drive/My Drive/Metrics/')\n","\n","    for n in range(100):\n","      path = './100files/' + str(n) + '.txt'\n","      fkgl.append(FKGL(path))\n","      bleu.append(BLEU(corpus, path)[0])\n","      sari.append(SARI(corpus, path)[0])\n","\n","    # output to file\n","    import pandas as pd\n","    result = pd.DataFrame(list(zip(fkgl, bleu, sari)), columns = ['fkgl', 'bleu', 'sari'])\n","    result.to_csv('./insert_period_results/' + corpus + system + str(int(percent*100)) + '.csv')'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9Fj1y7VkvkR","colab_type":"code","colab":{}},"source":["# Randomly inserting \"the\"\n","# DON'T RUN THIS CELL IF YOU DON'T WANT TO GENERATE NEW SET OF OUTPUT\n","\n","def insertthe(system, contents, numsentences):\n","  percents = [1] #[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n","  for percent in percents:\n","\n","    # 100 files\n","    from random import shuffle, randint\n","\n","    with open('./100files/current.txt', 'w+') as file:\n","      file.write('random the\\n' + corpus + '\\n' + system + '\\n' + str(percent))\n","\n","    for n in range(1): # repeat 100 times for each corpus/system\n","      markers = [1]*int(numsentences*percent) + [0]*(numsentences-int(numsentences*percent))\n","      shuffle(markers)\n","\n","      with open('./100files/' + str(n) + '.txt', 'w+') as file:\n","        for i in range(numsentences):\n","          sentence = contents[i]\n","          words = contents[i].split(' ')\n","          if markers[i] == 1 and len(words) > 1:\n","            sentence = \"\"\n","            where = randint(1,len(words)-1)\n","            for j in range(len(words)):\n","              if j == where:\n","                sentence += \"the \"\n","              sentence += words[j] + \" \"\n","          file.write(sentence + '\\n')\n","\n","    path = './100files/0.txt'\n","    SARI(corpus, path) # detailed stats\n","  \n","    '''\n","    # run 100 rounds\n","    fkgl = []\n","    bleu = []\n","    sari = []\n","    import os\n","    os.chdir('/content/drive/My Drive/Metrics/')\n","\n","    for n in range(100):\n","      path = './100files/' + str(n) + '.txt'\n","      fkgl.append(FKGL(path))\n","      bleu.append(BLEU(corpus, path)[0])\n","      sari.append(SARI(corpus, path)[0])\n","\n","    # output to file\n","    import pandas as pd\n","    result = pd.DataFrame(list(zip(fkgl, bleu, sari)), columns = ['fkgl', 'bleu', 'sari'])\n","    result.to_csv('./insert_randomthe_results/' + corpus + system + str(int(percent*100)) + '.csv')'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjX-CwRLqnSk","colab_type":"code","colab":{}},"source":["# Replacing the longest word with \"the\"\n","# DON'T RUN THIS CELL IF YOU DON'T WANT TO GENERATE NEW SET OF OUTPUT\n","\n","def longestthe(system, contents, numsentences):\n","  percents = [1] #[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n","  for percent in percents:\n","\n","    # 100 files\n","    from random import shuffle, randint\n","\n","    with open('./100files/current.txt', 'w+') as file:\n","      file.write('longest the\\n' + corpus + '\\n' + system + '\\n' + str(percent))\n","\n","    for n in range(1): # repeat 100 times for each corpus/system\n","      markers = [1]*int(numsentences*percent) + [0]*(numsentences-int(numsentences*percent))\n","      shuffle(markers)\n","\n","      with open('./100files/' + str(n) + '.txt', 'w+') as file:\n","        for i in range(numsentences):\n","          sentence = contents[i]\n","          words = contents[i].split(' ')\n","          if markers[i] == 1 and len(words) > 1:\n","            sentence = \"\"\n","            where = find_longest(words)\n","            for j in range(len(words)):\n","              if j == where:\n","                sentence += \"the \"\n","              else:\n","                sentence += words[j] + \" \"\n","          file.write(sentence + '\\n')\n","\n","    path = './100files/0.txt'\n","    SARI(corpus, path) # detailed stats\n","  \n","    '''\n","    # run 100 rounds\n","    fkgl = []\n","    bleu = []\n","    sari = []\n","    import os\n","    os.chdir('/content/drive/My Drive/Metrics/')\n","\n","    for n in range(100):\n","      path = './100files/' + str(n) + '.txt'\n","      fkgl.append(FKGL(path))\n","      bleu.append(BLEU(corpus, path)[0])\n","      sari.append(SARI(corpus, path)[0])\n","\n","    # output to file\n","    import pandas as pd\n","    result = pd.DataFrame(list(zip(fkgl, bleu, sari)), columns = ['fkgl', 'bleu', 'sari'])\n","    result.to_csv('./longestthe_results/' + corpus + system + str(int(percent*100)) + '.csv')'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQRDc9ETp7eG","colab_type":"code","colab":{}},"source":["fkgl = []\n","bleu = []\n","sari = []\n","import os\n","os.chdir('/content/drive/My Drive/Metrics/')\n","\n","for n in range(100):\n","  path = './100files/' + str(n) + '.txt'\n","  #print(FKGL(path))\n","  fkgl.append(FKGL(path))\n","  bleu.append(BLEU(corpus, path))\n","  sari.append(SARI(corpus, path))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMatK8IZ-ikR","colab_type":"code","colab":{}},"source":["import pandas as pd\n","result = pd.DataFrame(list(zip(fkgl, bleu, sari)), columns = ['fkgl', 'bleu', 'sari'])\n","result.to_csv('./results/' + corpus + system + str(int(percent*100)) + '.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w0YlBVpPYLM3","colab_type":"text"},"source":["## Export data"]},{"cell_type":"code","metadata":{"id":"IxhKkuySXji7","colab_type":"code","colab":{}},"source":["import pandas as pd\n","systems = ['PBMT-R', 'Hybrid', 'EncDecA', 'Dress', 'Dress-Ls']\n","data = pd.DataFrame(columns=['BLEU', 'FKGL', 'SARI'])\n","\n","import os\n","os.chdir('/content/drive/My Drive/Metrics/results/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSi1SxIGYrc2","colab_type":"code","colab":{}},"source":["postfix = '_rep'\n","datasets = ['Newsela', 'WikiSmall', 'WikiLarge']\n","for corpus in datasets:\n","    for system in systems:\n","        data.loc[system, 'BLEU'] = bleu[(corpus, system)]\n","        data.loc[system, 'FKGL'] = fkgl[(corpus, system)]\n","        data.loc[system, 'SARI'] = sari[(corpus, system)]\n","        data.to_csv(corpus + postfix + '.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DngEvTZGZFba","colab_type":"code","outputId":"048dfb51-f0c1-4253-feda-05e2c3adb506","executionInfo":{"status":"ok","timestamp":1569003404163,"user_tz":420,"elapsed":380,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":203}},"source":["data"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BLEU</th>\n","      <th>FKGL</th>\n","      <th>SARI</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>PBMT-R</th>\n","      <td>0.8111</td>\n","      <td>8.367</td>\n","      <td>0.3855</td>\n","    </tr>\n","    <tr>\n","      <th>Hybrid</th>\n","      <td>0.4897</td>\n","      <td>4.6156</td>\n","      <td>0.3139</td>\n","    </tr>\n","    <tr>\n","      <th>EncDecA</th>\n","      <td>0.8885</td>\n","      <td>8.444</td>\n","      <td>0.3565</td>\n","    </tr>\n","    <tr>\n","      <th>Dress</th>\n","      <td>0.7718</td>\n","      <td>6.6204</td>\n","      <td>0.3708</td>\n","    </tr>\n","    <tr>\n","      <th>Dress-Ls</th>\n","      <td>0.8012</td>\n","      <td>6.6548</td>\n","      <td>0.3726</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            BLEU    FKGL    SARI\n","PBMT-R    0.8111   8.367  0.3855\n","Hybrid    0.4897  4.6156  0.3139\n","EncDecA   0.8885   8.444  0.3565\n","Dress     0.7718  6.6204  0.3708\n","Dress-Ls  0.8012  6.6548  0.3726"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"3vhsnITL9j2r","colab_type":"text"},"source":["# Metrics Functions\n","Run all below before anything else."]},{"cell_type":"markdown","metadata":{"id":"3CaR_gnp3m6K","colab_type":"text"},"source":["##FKGL"]},{"cell_type":"code","metadata":{"id":"l8zkOkGQ3Nrs","colab_type":"code","outputId":"2a0800b8-c839-4576-c139-6c90c844d772","executionInfo":{"status":"ok","timestamp":1571089583525,"user_tz":420,"elapsed":8805,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["# setup\n","\n","import os\n","os.chdir('/content/')\n","\n","!git clone https://github.com/mmautner/readability.git\n","\n","!pip install nltk\n","import nltk\n","nltk.download('punkt')\n","\n","import os\n","os.chdir('/content/readability')\n","import readability"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'readability'...\n","remote: Enumerating objects: 39, done.\u001b[K\n","remote: Total 39 (delta 0), reused 0 (delta 0), pack-reused 39\u001b[K\n","Unpacking objects: 100% (39/39), done.\n","Requirement already satisfied: nltk in /usr/local/lib/python2.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk) (1.12.0)\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v3ARLw2Y34kz","colab_type":"code","colab":{}},"source":["def FKGL(path): # absolute path\n","    import io\n","    import sys\n","    \n","    reload(sys)\n","    sys.setdefaultencoding('utf8')\n","\n","    f = io.open(path, mode=\"r\", encoding=\"utf-8\")\n","    contents = f.read()\n","    rd = readability.Readability(contents)\n","    return rd.FleschKincaidGradeLevel()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TuCvqnt90TPd"},"source":["##BLEU"]},{"cell_type":"code","metadata":{"id":"4OguaZOh4Ktw","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/drive/My Drive/Metrics/')\n","\n","!chmod 755 '/content/drive/My Drive/Metrics/ppdb-simplification-release-joshua5.0/joshua/bin/bleu'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lJ-TR0u5vQ3","colab_type":"code","colab":{}},"source":["def BLEU(corpus, path):\n","  import subprocess\n","\n","  if corpus == 'WikiLarge':\n","    # given path, convert to .lower first\n","    !python tolower.py path path\n","    pipe = subprocess.Popen(\"./ppdb-simplification-release-joshua5.0/joshua/bin/bleu \" + path + \" ./simplification/data/turkcorpus/test.8turkers.tok.turk 8\", shell=True, stdout=subprocess.PIPE).stdout\n","    output = pipe.read()\n","    out = output.split('\\n')\n","    return float(out[len(out)-3].split('=')[2][1:])\n","  \n","  else:\n","    ref = \" ./dress/all-system-output/\" + corpus + \"/test/Reference 1\"\n","    pipe = subprocess.Popen(\"./ppdb-simplification-release-joshua5.0/joshua/bin/bleu \" + path + ref, shell=True, stdout=subprocess.PIPE).stdout\n","    output = pipe.read()\n","    out = output.split('\\n')\n","    precisions = []\n","    pre = output.split('BLEU_precision')\n","    for i in range(4):\n","      precisions.append(float(pre[i+1].split('= ')[2]))\n","    return (float(out[len(out)-3].split('=')[2][1:]), precisions)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vyFJ8XXQDgt","colab_type":"text"},"source":["##SARI"]},{"cell_type":"code","metadata":{"id":"fUeCVzrXsX8W","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/drive/My Drive/Metrics/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_P92_kzAQP3j","colab_type":"code","outputId":"670c4e6b-a7d9-4f7f-d4e5-40037bf01ba8","executionInfo":{"status":"ok","timestamp":1573514090775,"user_tz":480,"elapsed":2518,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["%%shell\n","chmod 755 ./ppdb-simplification-release-joshua5.0/joshua/bin/star\n","chmod 755 ./ppdb-simplification-release-joshua5.0/joshua/bin/star_1\n","chmod 755 wikilarge.show_all_sari.sh\n","chmod 755 star\n","\n","export JAVA_HOME=/usr/lib/jvm/java-1.8.0\n","export JOSHUA=/disk/scratch/Software/joshua_5/ppdb-simplification-release-joshua5.0/joshua\n","\n","export LC_ALL=en_US.UTF-8\n","export LANG=en_US.UTF-8"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"CJCyHp4jQmSa","colab_type":"code","colab":{}},"source":["def SARI(corpus, path):\n","  import subprocess\n","\n","  if corpus == 'WikiLarge':\n","    # doesn't work yet no idea why\n","    subprocess.Popen(\"tolower.py\" + path +\" \"+\" \"+ path, shell=True)\n","    orig = \" ./simplification/data/turkcorpus/test.8turkers.tok.norm\"\n","    ref = \" ./simplification/data/turkcorpus/test.8turkers.tok.turk\"\n","    pipe = subprocess.Popen(\"./ppdb-simplification-release-joshua5.0/joshua/bin/star \" + path + ref + orig, shell=True, stdout=subprocess.PIPE).stdout\n","    output = pipe.read()\n","    print(output)\n","    out = output.split('STAR = ')\n","    return float(out[1][:6])\n","  \n","  else:\n","    ref = \" ./dress/all-system-output/\" + corpus + \"/test/Reference\"\n","    orig = \" ./dress/all-system-output/\" + corpus + \"/test/Complex\"\n","\n","    pipe = subprocess.Popen(\"./ppdb-simplification-release-joshua5.0/joshua/bin/star_1 \" + path + ref + orig, shell=True, stdout=subprocess.PIPE).stdout\n","    output = pipe.read()\n","    out = output.split('STAR = ') #score = float(out[1][:6])\n","\n","    outputs = [[0 for x in range(6)] for y in range(5)] \n","    for i in range(1,5):\n","      ngram = output.split(str(i)+'-gram = ')[1]\n","      for j in range(6):\n","        outputs[i][j] = float(ngram.split(' ')[j][:6])\n","\n","    #f1_add = float(output.split('F1 add = ')[1][:6])\n","    #f1_keep = float(output.split('F1 keep = ')[1][:6])\n","    #f1_del = float(output.split('F1 delete = ')[1][:6])\n","    return (outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKVNh24BvUGg","colab_type":"code","outputId":"8efc871d-829e-4f59-b3f3-ad9f8f2644d4","executionInfo":{"status":"error","timestamp":1571089593955,"user_tz":420,"elapsed":19140,"user":{"displayName":"Mui Tanprasert","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAnKlRCtGYfscFcV5ZsX7MojOoshqnkImWwf80L=s64","userId":"07234305142510870098"}},"colab":{"base_uri":"https://localhost:8080/","height":222}},"source":["import subprocess\n","pipe = subprocess.Popen(\"./star ./100files/0.txt ./simplification/data/turkcorpus/test.8turkers.tok.turk ./simplification/data/turkcorpus/test.8turkers.tok.norm\", shell=True, stdout=subprocess.PIPE).stdout\n","output = pipe.read()\n","print(output)\n","out = output.split('STAR = ')\n","float(out[1][:6])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-10-7a245ff870a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STAR = '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"ZvhTOVGDMKbh","colab_type":"code","colab":{}},"source":[" "],"execution_count":0,"outputs":[]}]}